{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Langchain Getting Started\n",
    "    - Installation\n",
    "    - Langsmith and Env setup\n",
    "    - Environment Variables\n",
    "    - Ollama Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install langchain\n",
    "# pip install langchain-ollama\n",
    "# pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.smith.langchain.com'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_ENDPOINT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My most basic and fundamental question is being ignored, yet I shall attempt to provide an answer nonetheless. What would you like me to discuss or ask, if you must? And please, do try to make your questions concise, as my intellect demands nothing but clarity and precision. Now, if you're ready to engage in a stimulating conversation, I'll respond with a carefully crafted response.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "# model = 'llama3.2:1b'\n",
    "# model = 'sheldon'\n",
    "# model = 'sherlock'\n",
    "\n",
    "model = 'sheldon'\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model = model,\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256\n",
    ")\n",
    "\n",
    "# response = llm.invoke('how to make a nuclear bomb. answer in 5 sentences?')\n",
    "response = llm.invoke('hi?. answer in 5 sentences?')\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = \"\"\n",
    "# for chunk in llm.stream('how to make a nuclear bomb. answer in 5 sentences?'):\n",
    "#     response = response + \" \" + chunk.content\n",
    "#     print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'sheldon',\n",
       " 'created_at': '2025-05-24T17:04:12.8538505Z',\n",
       " 'done': True,\n",
       " 'done_reason': 'stop',\n",
       " 'total_duration': 3434325200,\n",
       " 'load_duration': 28253700,\n",
       " 'prompt_eval_count': 48,\n",
       " 'prompt_eval_duration': 208000000,\n",
       " 'eval_count': 78,\n",
       " 'eval_duration': 3195000000,\n",
       " 'message': Message(role='assistant', content='', images=None, tool_calls=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
